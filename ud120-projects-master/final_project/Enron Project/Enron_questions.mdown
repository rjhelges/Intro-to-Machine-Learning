Enron Submission Free-Response Questions

1.	The goal of this project is to use financial and email data from Enron to create a machine learning algorithm that can predict person’s of interest given certain features. The dataset had 146 starting data points with 21 features. The features were composed of various types of financial and email data. Examples of financial data include salary and bonuses and email data included to and from messages including to and from other person’s of interest. There was also a ‘poi’ feature that was used as a label to identify which employee was a person of interest. There were 18 data points that identified as being a person of interest.
There was one major outlier when looking at a salary v bonus graph. That outlier happened to be the ‘TOTAL’ field that came from the original spreadsheet. It was removed because it wasn’t deemed relevant to finding person’s of interest. There were other less obvious outliers, but they weren’t spreadsheet errors like ‘TOTAL’ was. They were employees and were kept in the dataset because they were relevant to the problem.

2.	The features I used for my algorithm were poi, bonus, salary, percent_to_poi, percent_from_poi, exercised_stock_options, total_payments and restricted_stock_deferred. Percent_to_poi and percent_from_poi were features I created. Instead of just the number of emails an employee may have sent to or received from a person of interest, I thought a percentage of total emails was a better representation of how much contact they had with person’s of interest. I thought this would be a better feature to identify person’s of interest with. I also created percent_stock_exercised feature which was exercised_stock_options divided by total_stock_value (Full disclosure: I don’t know a lot about financial data so I’m not even sure if this would make sense or not. In my head it did so I figured I’d try it). This feature didn’t prove to be a better indicator in identifying person’s of interest over exercised_stock_options. With it accuracy, precision, and recall were .86, .60, and .33 respectively compared to .88, .67, and .38 when I just used exercised_stock_options in place of it.
My feature selection process was just trial and error. I started out with poi, bonus, and salary and I would add or subtract values to see how they effected the algorithm’s score. Below are some of the features lists I tested and the subsequent F1 score using tester.py:

'poi','bonus', 'salary', 'percent_to_poi', 'percent_from_poi', 'exercised_stock_options': 0.486
'poi','bonus', 'percent_to_poi', 'percent_from_poi', 'exercised_stock_options': 0.494
'poi','bonus', 'salary', 'percent_to_poi', 'percent_from_poi', 'exercised_stock_options', 'total_payments', 'deferral_payments': 0.496
'poi','bonus', 'salary', 'percent_to_poi', 'percent_from_poi', 'exercised_stock_options', 'total_payments', 'expenses': 0.504
'poi','bonus', 'salary', 'percent_to_poi', 'percent_from_poi', 'exercised_stock_options', 'total_payments': 0.504
'poi','bonus', 'salary', 'percent_to_poi', 'percent_from_poi': 0.327
'poi','bonus', 'salary': 0.333

Feature scaling was not using for this algorithm.


3.	The algorithm I ended up using was KNeighborsClassifier. It is a fairly simple algorithm but it vastly outperformed all the other algorithms I tried. In total I tried GaussianNB, DecisionTreeClassifier, and RandomForestClassifier with the DecisionTreeClassifier being the next best one and the only other one I really dove deep into besides KNeighborsClassifier.
Once I dialed in the KNeighborsClassifier, I was never able to get close to its score with DecisionTreeClassifier. KNeighbors had an accuracy of 0.90 with a precision and recall of .70 and .39 respectively. The best I could get with DecisionTree was an accuracy of 0.82 and a precision and recall of .40 and .37 respectively.

4.	To tune the parameters of an algorithm means to change how your algorithm goes about classifying the data and can go a long way to improving your results. If you don’t tune your parameters well then your algorithm will give you bad results. You could pick the right algorithm for the dataset, but if you tune it wrong it won’t matter.
I first started doing trial and error with the parameters, but that proved to be a daunting and time consuming process. I then used GridSearchCV to tune the parameters and then displayed the parameters that yielded the best results. This ended being incredibly efficient and worthwhile. I ended up using the parameters algorithm=‘auto’, leaf_size=5, n_neighbors=3, and p=1 after using GridSearchCV.

5.	Validation is the process of splitting your dataset into 2 groups: a training set and a testing set. You then use your training set to fit your algorithm and the testing set to test and validate the results of your algorithm. If you train and test on the same dataset, then you can get into a case where you’ve overfit your algorithm to the data and the algorithm fails when used outside of the specific dataset. You also have to find the right balance between how much data you put into your training and test sets. There is a trade off between having enough data to fit to your algorithm and having enough data to properly test and validate your algorithm.
For validation I used train_test_split. This takes your dataset and splits it into two groups: a training set and a testing set. I used a testing set that was 40% of the total dataset. This way a majority of the dataset was used to train the algorithm, but I still had a good size testing set to use to validate the algorithm.

6.	The 3 evaluation metrics I used were accuracy, precision, and recall which the algorithm scored 0.897, 0.680, and 0.434 respectively. Also had a F1 score of 0.530. The F1 score is a weighted average of the precision and recall. Accuracy is the measure of how often the algorithm was able to correctly identify if an employee was or wasn’t a person of interest. Precision is a measure of how well your algorithm correctly identified a person of interest. So for my algorithm, if it identified an employee as a person of interest, it was right 69.7% of the time. Recall is the measure of how well your algorithm identified a person of interest given all the persons of interest. So for my algorithm, it was able to identify 39.5% of all persons of interest.